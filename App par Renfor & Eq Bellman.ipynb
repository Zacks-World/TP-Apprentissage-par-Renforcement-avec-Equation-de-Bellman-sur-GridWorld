{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53a14008-1999-4c08-993b-0a695d32b143",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0424a3e6-9123-4db9-824d-74f5282e5571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values\n",
      "[[0.81   0.9    1.     0.    ]\n",
      " [0.729  0.     0.9    0.    ]\n",
      " [0.6561 0.729  0.81   0.729 ]]\n",
      "Policy\n",
      "[['‚Üí' '‚Üí' '‚Üí' 'G']\n",
      " ['‚Üë' '‚ñà' '‚Üë' 'üî•']\n",
      " ['‚Üí' '‚Üí' '‚Üë' '‚Üê']]\n",
      "Rewards\n",
      "[[ 0.  0.  0.  1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Param√®tres\n",
    "gamma = 0.9\n",
    "rows, cols = 3, 4  # Taille de la grille\n",
    "goal_state = (0, 3)\n",
    "fire_state = (1, 3)\n",
    "obstacle = (1, 1)\n",
    "reward_goal = 1\n",
    "reward_fire = -1\n",
    "reward_step = 0  # Encourage le chemin le plus court\n",
    "\n",
    "# Initialisation des valeurs des √©tats et des r√©compenses\n",
    "V = np.zeros((rows, cols))\n",
    "policy = np.full((rows, cols),' ', dtype=str)\n",
    "rewards = np.full((rows, cols), reward_step, dtype=float)\n",
    "rewards[goal_state] = reward_goal\n",
    "rewards[fire_state] = reward_fire\n",
    "rewards[obstacle] = None  # Obstacle infranchissable\n",
    "\n",
    "# D√©placements possibles et leurs repr√©sentations\n",
    "actions = [(-1, 0), (1, 0), (0, -1), (0, 1)]  # Haut, Bas, Gauche, Droite\n",
    "action_symbols = {(-1, 0): '‚Üë', (1, 0): '‚Üì', (0, -1): '‚Üê', (0, 1): '‚Üí'}\n",
    "\n",
    "# Algorithme de Value Iteration\n",
    "def value_iteration(V, rewards, gamma,policy,iterations=100):\n",
    "    for _ in range(iterations):\n",
    "        new_V = np.copy(V)\n",
    "        new_policy=np.copy(policy)\n",
    "        for i in range(rows):\n",
    "            for j in range(cols):\n",
    "                if (i, j) in [goal_state, fire_state, obstacle]:  # √âtats terminaux ou obstacles\n",
    "                    continue\n",
    "                values = []\n",
    "                for action in actions:\n",
    "                    ni, nj = i + action[0], j + action[1]\n",
    "                    if 0 <= ni < rows and 0 <= nj < cols and (ni, nj) != obstacle:\n",
    "                        values.append((rewards[(ni, nj)] + gamma * V[ni, nj], action_symbols[action]))\n",
    "                if values:\n",
    "                    new_V[i, j], new_policy[i,j] = max(values)\n",
    "        V = new_V\n",
    "        policy=new_policy\n",
    "        policy[goal_state] = 'G'\n",
    "        policy[fire_state] = 'üî•'\n",
    "        policy[obstacle] = '‚ñà'\n",
    "    return V,policy\n",
    "\n",
    "# D√©termination de la meilleure action pour chaque √©tat\n",
    "def policy_extraction(V, rewards):\n",
    "    policy = np.full((rows, cols), ' ', dtype=str)\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            if (i, j) == goal_state:\n",
    "                policy[i, j] = 'G'\n",
    "            elif (i, j) == fire_state:\n",
    "                policy[i, j] = 'üî•'\n",
    "            elif (i, j) == obstacle:\n",
    "                policy[i, j] = '‚ñà'\n",
    "            else:\n",
    "                best_action = None\n",
    "                best_value = float('-inf')\n",
    "                for action in actions:\n",
    "                    ni, nj = i + action[0], j + action[1]\n",
    "                    if 0 <= ni < rows and 0 <= nj < cols and (ni, nj) != obstacle:\n",
    "                        value = rewards[(ni, nj)] + gamma * V[ni, nj]\n",
    "                        if value > best_value:\n",
    "                            best_value = value\n",
    "                            best_action = action\n",
    "                if best_action:\n",
    "                    policy[i, j] = action_symbols[best_action]\n",
    "    return policy\n",
    "\n",
    "# Ex√©cution des algorithmes\n",
    "V,policy = value_iteration(V, rewards, gamma,policy)\n",
    "\n",
    "print('Values')\n",
    "print(V)\n",
    "print('Policy')\n",
    "print(policy)\n",
    "print('Rewards')\n",
    "print(rewards)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
